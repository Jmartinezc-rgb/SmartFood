{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Entrenamiento de un modelo TransE con `new_triplets.csv`\n",
    "\n",
    "A continuación se muestra un ejemplo completo en Python (para ser ejecutado en un Notebook o script) que:\n",
    "\n",
    "1. Carga las tripletas del archivo `new_triplets.csv` (generado previamente).\n",
    "2. Divide el dataset en conjuntos de entrenamiento y test.\n",
    "3. Crea un modelo TransE usando la clase `ScoringBasedEmbeddingModel` de AmpliGraph con el parámetro `scoring_type=\"TransE\"`.\n",
    "4. Compila y entrena el modelo.\n",
    "5. Evalúa el modelo usando métricas de link prediction (por ejemplo, el MRR).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ampligraph.datasets import load_from_csv\n",
    "from ampligraph.evaluation import train_test_split_no_unseen, mrr_score, mr_score, hits_at_n_score\n",
    "from ampligraph.latent_features.models import ScoringBasedEmbeddingModel\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from ampligraph.latent_features.loss_functions import PairwiseLoss\n",
    "\n",
    "from ampligraph.evaluation import train_test_split_no_unseen, mrr_score, mr_score, hits_at_n_score\n",
    "from ampligraph.latent_features.models import ScoringBasedEmbeddingModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de tripletas en el CSV original: 3718024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7098/352491218.py:2: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"new_triplets.csv\")\n"
     ]
    }
   ],
   "source": [
    "# 1. Cargar el CSV completo de new_triplets.csv\n",
    "df = pd.read_csv(\"new_triplets.csv\")\n",
    "print(\"Total de tripletas en el CSV original:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de tripletas tras muestreo aleatorio: 10000\n"
     ]
    }
   ],
   "source": [
    "# 2. Seleccionar aleatoriamente 10,000 filas\n",
    "# Si el CSV tiene menos de 10,000 filas, tomará todas.\n",
    "n_muestras = min(len(df), 10000)\n",
    "df_sample = df.sample(n=n_muestras, random_state=42)\n",
    "print(\"Total de tripletas tras muestreo aleatorio:\", len(df_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Convertir las columnas a str para evitar conflictos de tipos\n",
    "df_sample['subject'] = df_sample['subject'].astype(str)\n",
    "df_sample['relation'] = df_sample['relation'].astype(str)\n",
    "df_sample['object'] = df_sample['object'].astype(str)\n",
    "\n",
    "# 3.1 Convertir a array de tripletas\n",
    "triples = df_sample[['subject', 'relation', 'object']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 9775\n",
      "Test size: 1000\n"
     ]
    }
   ],
   "source": [
    "# 4. Dividir en train/test (permitiendo duplicación para evitar entidades unseen)\n",
    "train_triples, test_triples = train_test_split_no_unseen(\n",
    "    triples,\n",
    "    test_size=0.1,\n",
    "    seed=42,\n",
    "    allow_duplication=True\n",
    ")\n",
    "print(\"Train size:\", len(train_triples))\n",
    "print(\"Test size:\", len(test_triples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuración de Hiperparámetros Sugerida para un Dataset de ~3.7M de Tripletas\n",
    "\n",
    "A continuación se propone una configuración de hiperparámetros como punto de partida para entrenar un modelo TransE con AmpliGraph en un dataset de aproximadamente 3.7 millones de tripletas.\n",
    "\n",
    "- **η (eta) = 10:** Número estándar de muestras negativas por triple.\n",
    "- **k = 200:** Aumentar la dimensión de los embeddings (de 100 a 200) puede ayudar a capturar mayor complejidad en un grafo grande, aunque aumenta el coste computacional.\n",
    "- **Batch size = 2000:** Usar un batch size mayor reduce el número de pasos por época y puede acelerar el entrenamiento, siempre que la memoria lo permita.\n",
    "- **Learning rate = 1e-4:** Una tasa de aprendizaje más baja puede estabilizar el entrenamiento en CPU, evitando oscilaciones en el loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Crear y compilar el modelo TransE con hiperparámetros ajustados\n",
    "# Ajustes para mejorar precisión:\n",
    "# - Incrementamos k de 100 a 200 para mayor capacidad\n",
    "# - Reducimos el learning rate de 1e-3 a 1e-4 para actualizaciones más suaves\n",
    "# - Aumentamos las épocas a 200 y usamos EarlyStopping\n",
    "model = ScoringBasedEmbeddingModel(\n",
    "    eta=10,              # Número de muestras negativas por triple\n",
    "    k=200,               # Dimensión de embeddings aumentada\n",
    "    scoring_type=\"ComplEx\",\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "# Usar PairwiseLoss con margen=1\n",
    "loss_obj = PairwiseLoss(loss_params={'margin': 1})\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=loss_obj,\n",
    "    run_eagerly=True  # Ejecuta en modo eager para facilitar la depuración\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "6/6 [==============================] - 1s 95ms/step - loss: 19550.0156\n",
      "Epoch 2/300\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 19548.7695\n",
      "Epoch 3/300\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 19547.6113\n",
      "Epoch 4/300\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 19546.4668\n",
      "Epoch 5/300\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 19545.3320\n",
      "Epoch 6/300\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 19544.1914\n",
      "Epoch 7/300\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 19543.0391\n",
      "Epoch 8/300\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 19541.8633\n",
      "Epoch 9/300\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 19540.6699\n",
      "Epoch 10/300\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 19539.4434\n",
      "Epoch 11/300\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 19538.1934\n",
      "Epoch 12/300\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 19536.9082\n",
      "Epoch 13/300\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 19535.5840\n",
      "Epoch 14/300\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 19534.2207\n",
      "Epoch 15/300\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 19532.8105\n",
      "Epoch 16/300\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 19531.3477\n",
      "Epoch 17/300\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 19529.8359\n",
      "Epoch 18/300\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 19528.2676\n",
      "Epoch 19/300\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 19526.6445\n",
      "Epoch 20/300\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 19524.9609\n",
      "Epoch 21/300\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 19523.2070\n",
      "Epoch 22/300\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 19521.3809\n",
      "Epoch 23/300\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 19519.4805\n",
      "Epoch 24/300\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 19517.5020\n",
      "Epoch 25/300\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 19515.4414\n",
      "Epoch 26/300\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 19513.2969\n",
      "Epoch 27/300\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 19511.0625\n",
      "Epoch 28/300\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 19508.7324\n",
      "Epoch 29/300\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 19506.3047\n",
      "Epoch 30/300\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 19503.7734\n",
      "Epoch 31/300\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 19501.1328\n",
      "Epoch 32/300\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 19498.3770\n",
      "Epoch 33/300\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 19495.5039\n",
      "Epoch 34/300\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 19492.5098\n",
      "Epoch 35/300\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 19489.3848\n",
      "Epoch 36/300\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 19486.1367\n",
      "Epoch 37/300\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 19482.7539\n",
      "Epoch 38/300\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 19479.2266\n",
      "Epoch 39/300\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 19475.5547\n",
      "Epoch 40/300\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 19471.7344\n",
      "Epoch 41/300\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 19467.7598\n",
      "Epoch 42/300\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 19463.6250\n",
      "Epoch 43/300\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 19459.3242\n",
      "Epoch 44/300\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 19454.8574\n",
      "Epoch 45/300\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 19450.2109\n",
      "Epoch 46/300\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 19445.3867\n",
      "Epoch 47/300\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 19440.3828\n",
      "Epoch 48/300\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 19435.1895\n",
      "Epoch 49/300\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 19429.8008\n",
      "Epoch 50/300\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 19424.2070\n",
      "Epoch 51/300\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 19418.4160\n",
      "Epoch 52/300\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 19412.4160\n",
      "Epoch 53/300\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 19406.1992\n",
      "Epoch 54/300\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 19399.7676\n",
      "Epoch 55/300\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 19393.1016\n",
      "Epoch 56/300\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 19386.2109\n",
      "Epoch 57/300\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 19379.0879\n",
      "Epoch 58/300\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 19371.7266\n",
      "Epoch 59/300\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 19364.1191\n",
      "Epoch 60/300\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 19356.2676\n",
      "Epoch 61/300\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 19348.1680\n",
      "Epoch 62/300\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 19339.8027\n",
      "Epoch 63/300\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 19331.1816\n",
      "Epoch 64/300\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 19322.2891\n",
      "Epoch 65/300\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 19313.1270\n",
      "Epoch 66/300\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 19303.6797\n",
      "Epoch 67/300\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 19293.9570\n",
      "Epoch 68/300\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 19283.9434\n",
      "Epoch 69/300\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 19273.6367\n",
      "Epoch 70/300\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 19263.0371\n",
      "Epoch 71/300\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 19252.1328\n",
      "Epoch 72/300\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 19240.9316\n",
      "Epoch 73/300\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 19229.4102\n",
      "Epoch 74/300\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 19217.5859\n",
      "Epoch 75/300\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 19205.4375\n",
      "Epoch 76/300\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 19192.9609\n",
      "Epoch 77/300\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 19180.1602\n",
      "Epoch 78/300\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 19167.0215\n",
      "Epoch 79/300\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 19153.5508\n",
      "Epoch 80/300\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 19139.7344\n",
      "Epoch 81/300\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 19125.5742\n",
      "Epoch 82/300\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 19111.0625\n",
      "Epoch 83/300\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 19096.1895\n",
      "Epoch 84/300\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 19080.9551\n",
      "Epoch 85/300\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 19065.3633\n",
      "Epoch 86/300\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 19049.4004\n",
      "Epoch 87/300\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 19033.0723\n",
      "Epoch 88/300\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 19016.3594\n",
      "Epoch 89/300\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 18999.2715\n",
      "Epoch 90/300\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 18981.8008\n",
      "Epoch 91/300\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 18963.9316\n",
      "Epoch 92/300\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 18945.6777\n",
      "Epoch 93/300\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 18927.0254\n",
      "Epoch 94/300\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 18907.9727\n",
      "Epoch 95/300\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 18888.5195\n",
      "Epoch 96/300\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 18868.6504\n",
      "Epoch 97/300\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 18848.3750\n",
      "Epoch 98/300\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 18827.6758\n",
      "Epoch 99/300\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 18806.5449\n",
      "Epoch 100/300\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 18785.0020\n",
      "Epoch 101/300\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 18763.0312\n",
      "Epoch 102/300\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 18740.6172\n",
      "Epoch 103/300\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 18717.7793\n",
      "Epoch 104/300\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 18694.4883\n",
      "Epoch 105/300\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 18670.7559\n",
      "Epoch 106/300\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 18646.5820\n",
      "Epoch 107/300\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 18621.9570\n",
      "Epoch 108/300\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 18596.8750\n",
      "Epoch 109/300\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 18571.3340\n",
      "Epoch 110/300\n",
      "6/6 [==============================] - 1s 111ms/step - loss: 18545.3242\n",
      "Epoch 111/300\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 18518.8574\n",
      "Epoch 112/300\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 18491.9199\n",
      "Epoch 113/300\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 18464.4941\n",
      "Epoch 114/300\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 18436.5957\n",
      "Epoch 115/300\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 18408.2207\n",
      "Epoch 116/300\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 18379.3691\n",
      "Epoch 117/300\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 18350.0156\n",
      "Epoch 118/300\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 18320.1699\n",
      "Epoch 119/300\n",
      "6/6 [==============================] - 1s 83ms/step - loss: 18289.8223\n",
      "Epoch 120/300\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 18258.9824\n",
      "Epoch 121/300\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 18227.6504\n",
      "Epoch 122/300\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 18195.7949\n",
      "Epoch 123/300\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 18163.4277\n",
      "Epoch 124/300\n",
      "6/6 [==============================] - 1s 94ms/step - loss: 18130.5547\n",
      "Epoch 125/300\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 18097.1699\n",
      "Epoch 126/300\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 18063.2559\n",
      "Epoch 127/300\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 18028.8164\n",
      "Epoch 128/300\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 17993.8555\n",
      "Epoch 129/300\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 17958.3730\n",
      "Epoch 130/300\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 17922.3359\n",
      "Epoch 131/300\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 17885.7754\n",
      "Epoch 132/300\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 17848.6758\n",
      "Epoch 133/300\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 17811.0684\n",
      "Epoch 134/300\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 17772.8906\n",
      "Epoch 135/300\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 17734.1562\n",
      "Epoch 136/300\n",
      "6/6 [==============================] - 1s 94ms/step - loss: 17694.8789\n",
      "Epoch 137/300\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 17655.0371\n",
      "Epoch 138/300\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 17614.6309\n",
      "Epoch 139/300\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 17573.6738\n",
      "Epoch 140/300\n",
      "6/6 [==============================] - 1s 83ms/step - loss: 17532.1406\n",
      "Epoch 141/300\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 17490.0586\n",
      "Epoch 142/300\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 17447.4160\n",
      "Epoch 143/300\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 17404.1777\n",
      "Epoch 144/300\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 17360.3516\n",
      "Epoch 145/300\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 17315.9355\n",
      "Epoch 146/300\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 17270.9590\n",
      "Epoch 147/300\n",
      "6/6 [==============================] - 1s 94ms/step - loss: 17225.3789\n",
      "Epoch 148/300\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 17179.2324\n",
      "Epoch 149/300\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 17132.4707\n",
      "Epoch 150/300\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 17085.1250\n",
      "Epoch 151/300\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 17037.1836\n",
      "Epoch 152/300\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 16988.6152\n",
      "Epoch 153/300\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 16939.4766\n",
      "Epoch 154/300\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 16889.7363\n",
      "Epoch 155/300\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 16839.3906\n",
      "Epoch 156/300\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 16788.4277\n",
      "Epoch 157/300\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 16736.8301\n",
      "Epoch 158/300\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 16684.6211\n",
      "Epoch 159/300\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 16631.7812\n",
      "Epoch 160/300\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 16578.3125\n",
      "Epoch 161/300\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 16524.2344\n",
      "Epoch 162/300\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 16469.5273\n",
      "Epoch 163/300\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 16414.1934\n",
      "Epoch 164/300\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 16358.2158\n",
      "Epoch 165/300\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 16301.5967\n",
      "Epoch 166/300\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 16244.2891\n",
      "Epoch 167/300\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 16186.4170\n",
      "Epoch 168/300\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 16127.8984\n",
      "Epoch 169/300\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 16068.7217\n",
      "Epoch 170/300\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 16008.8975\n",
      "Epoch 171/300\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 15948.4736\n",
      "Epoch 172/300\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 15887.3955\n",
      "Epoch 173/300\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 15825.7539\n",
      "Epoch 174/300\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 15763.5381\n",
      "Epoch 175/300\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 15700.8457\n",
      "Epoch 176/300\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 15637.6973\n",
      "Epoch 177/300\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 15574.1709\n",
      "Epoch 178/300\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 15510.3740\n",
      "Epoch 179/300\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 15446.3799\n",
      "Epoch 180/300\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 15382.2852\n",
      "Epoch 181/300\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 15318.1357\n",
      "Epoch 182/300\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 15253.9814\n",
      "Epoch 183/300\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 15189.9014\n",
      "Epoch 184/300\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 15125.9365\n",
      "Epoch 185/300\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 15062.0986\n",
      "Epoch 186/300\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 14998.4219\n",
      "Epoch 187/300\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 14934.9219\n",
      "Epoch 188/300\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 14871.6084\n",
      "Epoch 189/300\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 14808.5068\n",
      "Epoch 190/300\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 14745.6250\n",
      "Epoch 191/300\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 14683.0049\n",
      "Epoch 192/300\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 14620.6787\n",
      "Epoch 193/300\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 14558.5830\n",
      "Epoch 194/300\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 14496.7529\n",
      "Epoch 195/300\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 14435.2295\n",
      "Epoch 196/300\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 14373.9941\n",
      "Epoch 197/300\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 14313.0586\n",
      "Epoch 198/300\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 14252.4756\n",
      "Epoch 199/300\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 14192.1650\n",
      "Epoch 200/300\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 14132.1807\n",
      "Epoch 201/300\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 14072.5391\n",
      "Epoch 202/300\n",
      "6/6 [==============================] - 1s 95ms/step - loss: 14013.1855\n",
      "Epoch 203/300\n",
      "6/6 [==============================] - 1s 99ms/step - loss: 13954.1699\n",
      "Epoch 204/300\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 13895.4834\n",
      "Epoch 205/300\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 13837.1240\n",
      "Epoch 206/300\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 13779.0938\n",
      "Epoch 207/300\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 13721.3564\n",
      "Epoch 208/300\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 13663.9512\n",
      "Epoch 209/300\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 13606.9121\n",
      "Epoch 210/300\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 13550.1816\n",
      "Epoch 211/300\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 13493.8096\n",
      "Epoch 212/300\n",
      "6/6 [==============================] - 1s 83ms/step - loss: 13437.7510\n",
      "Epoch 213/300\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 13382.0254\n",
      "Epoch 214/300\n",
      "6/6 [==============================] - 1s 90ms/step - loss: 13326.6270\n",
      "Epoch 215/300\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 13271.5537\n",
      "Epoch 216/300\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 13216.8037\n",
      "Epoch 217/300\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 13162.3760\n",
      "Epoch 218/300\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 13108.2539\n",
      "Epoch 219/300\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 13054.4580\n",
      "Epoch 220/300\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 13000.9863\n",
      "Epoch 221/300\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 12947.8262\n",
      "Epoch 222/300\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 12894.9668\n",
      "Epoch 223/300\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 12842.4180\n",
      "Epoch 224/300\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 12790.1699\n",
      "Epoch 225/300\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 12738.2314\n",
      "Epoch 226/300\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 12686.5908\n",
      "Epoch 227/300\n",
      "6/6 [==============================] - 1s 93ms/step - loss: 12635.2568\n",
      "Epoch 228/300\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 12584.2383\n",
      "Epoch 229/300\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 12533.5449\n",
      "Epoch 230/300\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 12483.1367\n",
      "Epoch 231/300\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 12433.0596\n",
      "Epoch 232/300\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 12383.2803\n",
      "Epoch 233/300\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 12333.8047\n",
      "Epoch 234/300\n",
      "6/6 [==============================] - 1s 83ms/step - loss: 12284.6396\n",
      "Epoch 235/300\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 12235.7383\n",
      "Epoch 236/300\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 12187.1504\n",
      "Epoch 237/300\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 12138.8477\n",
      "Epoch 238/300\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 12090.8428\n",
      "Epoch 239/300\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 12043.1436\n",
      "Epoch 240/300\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 11995.6963\n",
      "Epoch 241/300\n",
      "6/6 [==============================] - 1s 95ms/step - loss: 11948.5391\n",
      "Epoch 242/300\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 11901.6582\n",
      "Epoch 243/300\n",
      "6/6 [==============================] - 1s 97ms/step - loss: 11855.0498\n",
      "Epoch 244/300\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 11808.7158\n",
      "Epoch 245/300\n",
      "6/6 [==============================] - 1s 99ms/step - loss: 11762.6025\n",
      "Epoch 246/300\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 11716.7979\n",
      "Epoch 247/300\n",
      "6/6 [==============================] - 1s 87ms/step - loss: 11671.2285\n",
      "Epoch 248/300\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 11625.9238\n",
      "Epoch 249/300\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 11580.8984\n",
      "Epoch 250/300\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 11536.0879\n",
      "Epoch 251/300\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 11491.5469\n",
      "Epoch 252/300\n",
      "6/6 [==============================] - 1s 106ms/step - loss: 11447.2676\n",
      "Epoch 253/300\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 11403.2168\n",
      "Epoch 254/300\n",
      "6/6 [==============================] - 1s 103ms/step - loss: 11359.4219\n",
      "Epoch 255/300\n",
      "6/6 [==============================] - 1s 91ms/step - loss: 11315.8965\n",
      "Epoch 256/300\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 11272.6309\n",
      "Epoch 257/300\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 11229.6201\n",
      "Epoch 258/300\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 11186.8779\n",
      "Epoch 259/300\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 11144.3936\n",
      "Epoch 260/300\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 11102.1885\n",
      "Epoch 261/300\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 11060.2441\n",
      "Epoch 262/300\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 11018.5625\n",
      "Epoch 263/300\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 10977.1494\n",
      "Epoch 264/300\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 10936.0020\n",
      "Epoch 265/300\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 10895.1357\n",
      "Epoch 266/300\n",
      "6/6 [==============================] - 1s 92ms/step - loss: 10854.5410\n",
      "Epoch 267/300\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 10814.2285\n",
      "Epoch 268/300\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 10774.1914\n",
      "Epoch 269/300\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 10734.4248\n",
      "Epoch 270/300\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 10694.9238\n",
      "Epoch 271/300\n",
      "6/6 [==============================] - 1s 83ms/step - loss: 10655.6992\n",
      "Epoch 272/300\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 10616.7549\n",
      "Epoch 273/300\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 10578.0684\n",
      "Epoch 274/300\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 10539.6631\n",
      "Epoch 275/300\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 10501.5293\n",
      "Epoch 276/300\n",
      "6/6 [==============================] - 1s 88ms/step - loss: 10463.6660\n",
      "Epoch 277/300\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 10426.0537\n",
      "Epoch 278/300\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 10388.7285\n",
      "Epoch 279/300\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 10351.6514\n",
      "Epoch 280/300\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 10314.8369\n",
      "Epoch 281/300\n",
      "6/6 [==============================] - 1s 89ms/step - loss: 10278.2979\n",
      "Epoch 282/300\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 10241.9941\n",
      "Epoch 283/300\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 10205.9473\n",
      "Epoch 284/300\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 10170.1465\n",
      "Epoch 285/300\n",
      "6/6 [==============================] - 0s 82ms/step - loss: 10134.6201\n",
      "Epoch 286/300\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 10099.3096\n",
      "Epoch 287/300\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 10064.2363\n",
      "Epoch 288/300\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 10029.4355\n",
      "Epoch 289/300\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 9994.8604\n",
      "Epoch 290/300\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 9960.5127\n",
      "Epoch 291/300\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 9926.4092\n",
      "Epoch 292/300\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 9892.5332\n",
      "Epoch 293/300\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 9858.8828\n",
      "Epoch 294/300\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 9825.4609\n",
      "Epoch 295/300\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 9792.2822\n",
      "Epoch 296/300\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 9759.3096\n",
      "Epoch 297/300\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 9726.5781\n",
      "Epoch 298/300\n",
      "6/6 [==============================] - 1s 84ms/step - loss: 9694.0527\n",
      "Epoch 299/300\n",
      "6/6 [==============================] - 0s 78ms/step - loss: 9661.7373\n",
      "Epoch 300/300\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 9629.6338\n"
     ]
    }
   ],
   "source": [
    "# 6. Entrenar el modelo con EarlyStopping para evitar overfitting\n",
    "history = model.fit(\n",
    "    train_triples,\n",
    "    batch_size=2000,     # Batch size mayor para acelerar el entrenamiento\n",
    "    epochs=300,          # Entrenar por 200 épocas (EarlyStopping se encargará de parar si no mejora)\n",
    "    verbose=True,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 2s 173ms/step\n",
      "=== Evaluación TransE - Corrupción de S + O ===\n",
      "MRR(s): 0.0004\n",
      "MR(s): 5930.03\n",
      "Hits@1(s): 0.0000\n",
      "Hits@10(s): 0.0000\n",
      "Hits@100(s): 0.0000\n"
     ]
    }
   ],
   "source": [
    "# 7. Evaluar el modelo (se evalúa corrompiendo sujeto y objeto por separado)\n",
    "ranks_so = model.evaluate(\n",
    "    test_triples,\n",
    "    batch_size=100,\n",
    "    corrupt_side='s+o'\n",
    ")\n",
    "\n",
    "# Calcular métricas\n",
    "mrr_so = mrr_score(ranks_so)\n",
    "mr_so = mr_score(ranks_so)\n",
    "hits1_so = hits_at_n_score(ranks_so, 1)\n",
    "hits10_so = hits_at_n_score(ranks_so, 10)\n",
    "hits100_so = hits_at_n_score(ranks_so, 100)\n",
    "\n",
    "\n",
    "print(\"=== Evaluación TransE - Corrupción de S + O ===\")\n",
    "print(f\"MRR(s): {mrr_so:.4f}\")\n",
    "print(f\"MR(s): {mr_so:.2f}\")\n",
    "print(f\"Hits@1(s): {hits1_so:.4f}\")\n",
    "print(f\"Hits@10(s): {hits10_so:.4f}\")\n",
    "print(f\"Hits@100(s): {hits100_so:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumen del Entrenamiento y Evaluación del Modelo TransE\n",
    "\n",
    "## Configuración y Entrenamiento\n",
    "- **Dataset:** Se usaron 100,000 muestras (tripletas) extraídas aleatoriamente de `new_triplets.csv`.\n",
    "- **Modelo:** Se entrenó un modelo TransE usando la clase `ScoringBasedEmbeddingModel`.\n",
    "- **Hiperparámetros:**\n",
    "  - **Número de muestras negativas (η):** 10\n",
    "  - **Dimensión de los embeddings (k):** 200\n",
    "  - **Learning Rate:** 1e-4 (para actualizaciones más suaves)\n",
    "  - **Batch Size:** 2000\n",
    "  - **Épocas:** 200 (con EarlyStopping implementado, aunque se completaron todas las épocas en este resumen)\n",
    "- **Pérdida (loss):**\n",
    "  - Inicialmente, el loss era de aproximadamente 17,922.\n",
    "  - Tras 200 épocas, el loss descendió a alrededor de 1,113.\n",
    "  - La curva de pérdida mostró una disminución constante, lo que indica que el modelo está aprendiendo la tarea de embebido de relaciones.\n",
    "\n",
    "## Evaluación del Modelo\n",
    "\n",
    "Se evaluó el modelo separadamente en dos escenarios:\n",
    "  \n",
    "### Corrupción de Sujeto\n",
    "- **MRR (Mean Reciprocal Rank):** 0.0001  \n",
    "- **MR (Mean Rank):** 43,610.73  \n",
    "- **Hits@1:** 0.0000  \n",
    "- **Hits@10:** 0.0000  \n",
    "- **Hits@100:** 0.0002  \n",
    "\n",
    "### Corrupción de Objeto\n",
    "- **MRR (Mean Reciprocal Rank):** 0.0585  \n",
    "- **MR (Mean Rank):** 1,650.38  \n",
    "- **Hits@1:** 0.0000  \n",
    "- **Hits@10:** 0.1932  \n",
    "- **Hits@100:** 0.4782  \n",
    "\n",
    "### Métrica Promedio\n",
    "- **Promedio MRR (sujeto + objeto):** 0.0293\n",
    "\n",
    "## Interpretación y Conclusiones\n",
    "\n",
    "- **Reducción del Loss:**  \n",
    "  La pérdida disminuyó significativamente (de ~17,922 a ~1,113) a lo largo de 200 épocas, lo que es una señal positiva de que el modelo está aprendiendo a diferenciar entre tripletas positivas y negativas.\n",
    "\n",
    "- **Desempeño en la Predicción de Entidades:**  \n",
    "  - **Corrupción de Sujeto:**  \n",
    "    El MRR es prácticamente 0 y el Mean Rank es muy alto (~43,610), lo que sugiere que el modelo tiene serias dificultades para predecir correctamente el sujeto.  \n",
    "  - **Corrupción de Objeto:**  \n",
    "    El MRR es algo mayor (0.0585) y el Mean Rank es considerablemente menor (~1,650), lo que indica que el modelo predice algo mejor el objeto de la tripleta.  \n",
    "  - La diferencia entre ambos lados sugiere que el modelo está aprendiendo de forma desigual, con una predicción muy pobre en el lado del sujeto.\n",
    "\n",
    "- **Baja Precisión General:**  \n",
    "  Un promedio MRR de 0.0293 es bajo, lo que implica que, en promedio, la entidad correcta aparece muy abajo en el ranking de predicción. Además, los valores de Hits@N son muy bajos, especialmente en Hits@1 y Hits@10, lo que indica que el modelo rara vez posiciona correctamente la entidad en las primeras posiciones.\n",
    "\n",
    "## Sugerencias para Mejorar el Modelo\n",
    "\n",
    "1. **Aumentar la Dimensión de los Embeddings:**  \n",
    "   Probar con un valor mayor a `k=200` para capturar representaciones más ricas del grafo.\n",
    "\n",
    "2. **Ajustar la Tasa de Aprendizaje:**  \n",
    "   Considerar un `learning_rate` aún más bajo (por ejemplo, 1e-5 o 1e-4) para permitir actualizaciones más finas, especialmente si se observa que el loss se estabiliza sin bajar lo suficiente.\n",
    "\n",
    "3. **Aumentar el Número de Épocas:**  \n",
    "   Si los recursos lo permiten, entrenar por más épocas podría ayudar, siempre vigilando las métricas en un conjunto de validación para evitar overfitting.\n",
    "\n",
    "4. **Revisar la Distribución de Datos:**  \n",
    "   Analizar la frecuencia de aparición de entidades y relaciones. Si existen muchas entidades con baja frecuencia, esto podría dificultar la generalización del modelo. Considerar técnicas de filtrado o muestreo equilibrado.\n",
    "\n",
    "5. **Explorar Otros Modelos:**  \n",
    "   Dada la asimetría en la predicción (sujeto vs. objeto), probar otros modelos de embeddings (como ComplEx, DistMult o RotatE) podría ofrecer un desempeño mejor en ciertas tareas de link prediction.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ampligraph_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
