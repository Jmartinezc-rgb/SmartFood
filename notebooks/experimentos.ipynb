{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Descarga y preparación del Dataset\n",
    "\n",
    "Este dataset contiene:\n",
    "\n",
    "`RAW_recipes.csv`: información de cada receta (ID, nombre, lista de ingredientes, tiempo de cocción, etc.).\n",
    "\n",
    "`RAW_interactions.csv`: reseñas de usuarios (ID de receta, ID de usuario, calificación, comentarios).\n",
    "\n",
    "https://www.kaggle.com/datasets/shuyangli94/food-com-recipes-and-user-interactions?resource=download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creación de un mini-Knowledge Graph\n",
    "## 2.1 Diseñar la estructura del grafo\n",
    "\n",
    "Para usar `AmpliGraph`, necesitamos tripletas (head, relation, tail). Un esquema sencillo podría ser:\n",
    "\n",
    "Receta →→ has_ingredient →→ Ingrediente\n",
    "Usuario →→ rated →→ Receta\n",
    "\n",
    "(Opcional) Receta →→ belongs_to_cuisine →→ Cocina (si clasificar o extraer la información de la columna tags).\n",
    "\n",
    "Ejemplo\n",
    "\"recipe_123\", \"has_ingredient\", \"tomato\"\n",
    "\"user_99\", \"rated\", \"recipe_123\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Extraer las tripletas de ingredientes\n",
    "\n",
    "Carga el CSV RAW_recipes.csv. Observa que en la columna ingredients tienes una lista (o cadena) con los ingredientes.\n",
    "\n",
    "Por cada receta: parsea su lista de ingredientes y genera tripletas con la relación \"has_ingredient\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_recipes = pd.read_csv(\"foodcom_data/RAW_recipes.csv\")\n",
    "\n",
    "# Cada fila tiene 'id' (recipe ID), 'ingredients' (lista en str)\n",
    "# Supongamos que se ven como \"['tomato', 'onion', 'salt']\"\n",
    "# Con eval() o ast.literal_eval() convertimos el string a lista Python\n",
    "import ast\n",
    "\n",
    "triplets = []\n",
    "for _, row in df_recipes.iterrows():\n",
    "    recipe_id = f\"recipe_{row['id']}\"\n",
    "    ing_list = ast.literal_eval(row['ingredients'])  # de string a lista\n",
    "    for ing in ing_list:\n",
    "        # Normalizar ingrediente (ej. poner en minúsculas, quitar espacios)\n",
    "        ing_norm = ing.strip().lower().replace(\" \", \"_\")\n",
    "        triplets.append((recipe_id, \"has_ingredient\", f\"ingredient_{ing_norm}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Extraer las tripletas de usuarios que califican recetas\n",
    "\n",
    "Carga el CSV RAW_interactions.csv, que contiene user_id, recipe_id y rating.\n",
    "\n",
    "Crea tripletas (user_X, \"rated\", recipe_Y). Si quieres, puedes incluir la calificación en la relación (aunque AmpliGraph maneja mejor relaciones categóricas). Una forma es poner la calificación como parte del “predicate” o tener un “rated_5stars” (pero esto multiplica relaciones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interactions = pd.read_csv(\"foodcom_data/RAW_interactions.csv\")\n",
    "\n",
    "for _, row in df_interactions.iterrows():\n",
    "    user_id = f\"user_{row['user_id']}\"\n",
    "    recipe_id = f\"recipe_{row['recipe_id']}\"\n",
    "    triplets.append((user_id, \"rated\", recipe_id))\n",
    "    # Opcional: si quieres, añade algo como: \n",
    "    # (user_id, f\"rated_{int(row['rating'])}_stars\", recipe_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Guardar las tripletas a CSV\n",
    "\n",
    "Para usar AmpliGraph, lo más sencillo es tener un CSV con columnas head, relation, tail. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"foodcom_data/graph_triplets.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    for h, r, t in triplets:\n",
    "        writer.writerow([h, r, t])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos un archivo graph_triplets.csv con todo nuestro “Knowledge Graph” en formato (head, relation, tail)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Una vez conseguimos el csv lo cargamos con AmpliGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 19:03:36.762546: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-03 19:03:36.763786: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-03 19:03:36.786839: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-03 19:03:36.787262: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-03 19:03:37.153048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "load_from_csv() got an unexpected keyword argument 'folder_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mampligraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split_no_unseen\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Cargar tripletas\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m triples \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKGE/foodcom_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgraph_triplets.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal triplets loaded:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(triples))\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Dividir en train/test sin introducir entidades desconocidas\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: load_from_csv() got an unexpected keyword argument 'folder_path'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ampligraph.datasets import load_from_csv\n",
    "from ampligraph.evaluation import train_test_split_no_unseen\n",
    "\n",
    "# Cargar tripletas\n",
    "triples = load_from_csv(\n",
    "    directory_path=\"foodcom_data\",\n",
    "    file_name=\"graph_triplets.csv\",\n",
    "    sep=\",\"\n",
    ")\n",
    "print(\"Total triplets loaded:\", len(triples))\n",
    "\n",
    "# Dividir en train/test sin introducir entidades desconocidas\n",
    "train_triples, test_triples = train_test_split_no_unseen(\n",
    "    triples, \n",
    "    test_size=0.2, \n",
    "    seed=42\n",
    ")\n",
    "print(\"Train size:\", len(train_triples))\n",
    "print(\"Test size:\", len(test_triples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Carga y división del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mampligraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_from_csv\n\u001b[1;32m      3\u001b[0m triples \u001b[38;5;241m=\u001b[39m load_from_csv(\n\u001b[1;32m      4\u001b[0m     directory_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfoodcom_data\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     file_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_graph_triplets.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# tu archivo\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/ampligraph_env/lib/python3.8/site-packages/ampligraph/__init__.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpkg_resources\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     14\u001b[0m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mlogging\u001b[38;5;241m.\u001b[39mset_verbosity(tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mlogging\u001b[38;5;241m.\u001b[39mERROR)\n\u001b[1;32m     16\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2.1.0\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from ampligraph.datasets import load_from_csv\n",
    "\n",
    "triples = load_from_csv(\n",
    "    directory_path=\"foodcom_data\",\n",
    "    file_name=\"my_graph_triplets.csv\",  # tu archivo\n",
    "    sep=\",\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.evaluation import train_test_split_no_unseen\n",
    "\n",
    "train_triples, test_triples = train_test_split_no_unseen(\n",
    "    triples,\n",
    "    test_size=0.2,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Entrenamiento de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/javimc/anaconda3/envs/ampligraph_env/bin/python\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 18:39:13.306294: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-26 18:39:13.307471: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-26 18:39:13.333638: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-26 18:39:13.334244: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-26 18:39:13.784392: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TransE' from 'ampligraph.latent_features' (/home/javimc/anaconda3/envs/ampligraph_env/lib/python3.8/site-packages/ampligraph/latent_features/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mampligraph\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(ampligraph\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mampligraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlatent_features\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransE\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTodo OK\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TransE' from 'ampligraph.latent_features' (/home/javimc/anaconda3/envs/ampligraph_env/lib/python3.8/site-packages/ampligraph/latent_features/__init__.py)"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "import ampligraph\n",
    "print(ampligraph.__version__)\n",
    "from ampligraph.latent_features import TransE\n",
    "print(\"Todo OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TransE' from 'ampligraph.latent_features' (/home/javimc/anaconda3/envs/ampligraph_env/lib/python3.8/site-packages/ampligraph/latent_features/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mampligraph\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(ampligraph\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mampligraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlatent_features\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransE\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m TransE(\n\u001b[1;32m      7\u001b[0m     batches_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m,\n\u001b[1;32m      8\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     17\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TransE' from 'ampligraph.latent_features' (/home/javimc/anaconda3/envs/ampligraph_env/lib/python3.8/site-packages/ampligraph/latent_features/__init__.py)"
     ]
    }
   ],
   "source": [
    "import ampligraph\n",
    "print(ampligraph.__version__)\n",
    "\n",
    "from ampligraph.latent_features import TransE\n",
    "\n",
    "model = TransE(\n",
    "    batches_count=300,\n",
    "    seed=0,\n",
    "    epochs=100,\n",
    "    k=100,\n",
    "    eta=10,\n",
    "    optimizer='adam',\n",
    "    optimizer_params={'lr':1e-3},\n",
    "    loss='pairwise',\n",
    "    loss_params={'margin':1},\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluación inicial\n",
    "\n",
    "Con evaluate_performance podemos calcular métricas de link prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.evaluation import evaluate_performance, mrr_score\n",
    "\n",
    "ranks = evaluate_performance(\n",
    "    test_triples,\n",
    "    model=model,\n",
    "    filter_triples=train_triples,\n",
    "    use_default_protocol=True\n",
    ")\n",
    "\n",
    "mrr = mrr_score(ranks)\n",
    "print(\"MRR:\", mrr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Recomendador de recetas\n",
    "## 7.1 Extracción de embeddings de recetas\n",
    "\n",
    "Supongamos que quieres recomendar recetas similares según sus embeddings. Primero, obtén todos los IDs de recetas (por ejemplo, los que empiezan por \"recipe_\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_entities = set([row[0] for row in triples] + [row[2] for row in triples])\n",
    "recipe_ids = [e for e in all_entities if e.startswith(\"recipe_\")]\n",
    "\n",
    "recipe_embeddings = model.get_embeddings(recipe_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Cálculo de similitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def recommend_similar_recipes(ref_recipe, top_n=5):\n",
    "    ref_emb = model.get_embeddings([ref_recipe])[0]\n",
    "    similarities = []\n",
    "    for i, rid in enumerate(recipe_ids):\n",
    "        emb = recipe_embeddings[i]\n",
    "        sim = cosine_similarity(ref_emb, emb)\n",
    "        similarities.append((rid, sim))\n",
    "    \n",
    "    # Ordenar de mayor a menor\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    # Excluir la propia receta\n",
    "    filtered = [(r, s) for (r, s) in similarities if r != ref_recipe]\n",
    "    return filtered[:top_n]\n",
    "\n",
    "# Ejemplo\n",
    "similar = recommend_similar_recipes(\"recipe_123\", top_n=5)\n",
    "print(\"Recetas similares a recipe_123:\", similar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Posibles mejoras\n",
    "\n",
    "Relaciones de rating: si quieres usar la información de rating de forma más fina, podrías crear relaciones diferenciadas (rated_5stars, rated_4stars, etc.) o usar la calificación en la fase de recomendación (filtrando recetas con baja puntuación).\n",
    "Filtro colaborativo: si tienes usuarios, podrías sugerir recetas basadas en embeddings de usuarios y su proximidad a otras recetas.\n",
    "Información extra: el dataset de Food.com tiene también tags o descripciones que podrías transformar en relaciones (por ejemplo, recipe_X belongs_to_cuisine Y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ampligraph_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
